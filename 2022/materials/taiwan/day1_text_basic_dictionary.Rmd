---
title: "SICSS Taiwan Text Analysis in R Code Along"
output:
  html_document:
    toc: yes
    toc_depth: 5
---

## Introduction

This material is adapted from Chris Bail's and SICSS-Howard 2022 tutorial. 

Among the most basic forms of quantitative text analysis are word-counting techniques and dictionary-based methods. This tutorial will cover both of these topics, as well as sentiment analysis, which is a form of dictionary-based text analysis. This tutorial assumes basic knowledge about R and other skills described in previous tutorials at the link above.

## Who is this code along for?
- Folks who haven't worked with tokenization, finding out word frequency, tf-idf, sentiment analysis 
- Folks who want to try out R's tidytext package

## How to use this code along
- Run the code chunks
- See if the output is what you expected
- Ask yourself what new function are you encountering. Does this make sense?
- Try answering the questions throughout the markdown file
- Ask one another for help
- Come to main session for help

## Word Counting

In the early days of quantitative text analysis, word-frequency counting in texts was a common mode of analysis. In this section, we'll learn a few basic techniques for counting word frequencies and visualizing them. We're going to work within the `tidytext` framework, so if you need a refresher on that, see my previous tutorial entitled "Basic Text Analysis in R."

Let's begin by loading the Trump tweets we extracted in a previous tutorial and transform them into `tidytext` format:

```{r, message=FALSE, warning=FALSE}
load(url("https://github.com/sicsshowardmathematica/day3_twitter_data/blob/main/VPHarris_Tweets.Rdata?raw=true"))
library(tidytext)
library(dplyr)
tidy_kamala_tweets<- VPHarris_Tweets %>%
    select(created_at,text) %>%
    unnest_tokens("word", text)
```

*Question 1* What would be the unit of analysis of the dataframe **VPHarris_Tweets**? What about **tidy_kamala_tweets**?

*Question 2* What is the function **unnest_tokens** doing? 

*Question 3* After removing stop words (frequent words such as "the", and "and") as well as other unmeaningful words (e.g. https), please count the word frequency in the corpus. 

```{r, message=FALSE, warning=FALSE}

data("stop_words")

top_words<-
   tidy_kamala_tweets %>%
  ## how to remove stop words??
    ## Please fill this out (or uncomment the following code)
  
##      anti_join(stop_words) %>%
##        filter(!( word=="https"|
##           word=="rt"|
##                 word=="t.co"|
##                 word=="amp")) %>%
##            count(word) %>%
##              arrange(desc(n))

  
  
  
  
  
    
```

Now let's make a graph of the top 20 words

```{r, message=FALSE, warnings=FALSE}
library(ggplot2)
top_words %>%
  slice(1:20) %>%
    ggplot(aes(x=reorder(word, -n), y=n, fill=word))+
      geom_bar(stat="identity")+
        theme_minimal()+
        theme(axis.text.x = 
            element_text(angle = 60, hjust = 1, size=13))+
        theme(plot.title = 
            element_text(hjust = 0.5, size=18))+
          ylab("Frequency")+
          xlab("")+
          ggtitle("Most Frequent Words in Trump Tweets")+
          guides(fill=FALSE)
```

## ngram

*Question 4* After removing the same set of stop words, including the twitter url, what would be the most bigram that appears in this Corpus?

See this section for hint https://www.tidytextmining.com/ngrams.html 
Use **tidyr::separate**
```{r}

# Step 1: Start with the tweet corpus
# Step 2: tokenization with bigram using tidytext::unnest_tokens function
# Step 3: customize your stop words to include things like "https"
```

## tf-idf

Though we have already removed very common "stop words" from our analysis, it is common practice in quantitative text analysis to identify unusual words that might set one document apart from the others (this will become particularly important when we get to more advanced forms of pattern recognition in text later on). 

<br><br>
We can calculate the tf-idf for the Trump tweets databased in `tidytext` as follows:

*Question 5*: how would you explain tf-idf? Unusual words, compared with what?

```{r, message=FALSE, warning=FALSE}

tidy_kamala_tfidf<- VPHarris_Tweets %>%
    select(created_at,text) %>%
      unnest_tokens("word", text) %>%
        anti_join(stop_words) %>%
           count(word, created_at) %>%
              bind_tf_idf(word, created_at, n)
```

Now let's see what the most unusual words are:

```{r}
top_tfidf<-tidy_kamala_tfidf %>%
  arrange(desc(tf_idf))

top_tfidf$word[1]

```

The tfidf increases the more a term appears in a document but it is negatively weighted by the overall frequency of terms across all documents in the dataset or Corpus. In simpler terms, the tf-idf helps us capture which words are not only important within a given document but also distinctive vis-a-vis the broader corpus or tidytext dataset.

## Dictionary-Based Quantitative Text Analysis

Though word frequency counts and tf-idf can be an informative way to examine text-based data, another very popular techniques involves counting the number of words that appear in each document that have been assigned a particular meaning or value to the researcher. There are numerous examples that we shall discuss below--- some of which are more sophisticated than others.

**Creating your own dictionary**

To begin, let's make our own dictionary of terms we want to examine from the Trump tweet dataset. Suppose we are doing a study of economic issues, and want to subset those tweets that contain words associated with the economy. To do this, we could first create a list or "dictionary" or terms that are associated with the economy.

```{r}
economic_dictionary<-c("economy","unemployment","trade","tariffs")
```


*Question 6*: Create another customized list of dictionary
```{r}
special_topic <- c()
```

Having created a very simple/primitive dictionary, we can now subset the parts of our tidytext dataframe that contain these words using the `str_detect` function within Hadley Wickham's `stringr` package: 

```{r}
library(stringr)
economic_tweets<-VPHarris_Tweets[str_detect(VPHarris_Tweets$text, paste(economic_dictionary, collapse="|")),]

## change the following line
topic_tweets <- 

```

## Sentiment Analysis

The example above was somewhat arbitrary and mostly designed to introduce you to the concept of dictionary-base text analysis. The list of economic terms that I came up with was very ad hoc---and though the tweets identified above each mention the economy, there are probably many more tweets in our dataset that reference economic issues that do not include the words I identified.

Dictionary-based approaches are often most useful when a high-quality dictionary is available that is of interest to the researcher or analyst. One popular type of dictionary is a sentiment dictionary which can be used to assess the valence of a given text by searching for words that describe affect or opinion. Some of these dictionaries are created by examining comparing text-based evaluations of products in online forums to ratings systems. Others are created via systematic observation of people writing who have been primed to write about different emotions.

Let's begin by examining some of the sentiment dictionaries that are built into `tidytext.` These include the `afinn` which includes a list of sentiment-laden words that appeared in [Twitter discussions of climate change](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6006/pdf/imm6006.pdf); `bing` which includes sentiemnt words identified on online forums; and `nrc` which is a dictionary that was created by having workers on Amazon mechanical Turk code the emotional valence of a [long list of terms](https://arxiv.org/pdf/1308.6297.pdf). These algorithims can produce very different results since they were created using very different datasets (meaning they identify sentiment laden words using different corpora). 

*Question 7*: Put on your critique hat, skim through the paper about how the sentiment is created. What do you think? Are you convinced? Will you use it?

Each of these dictionaries only describe sentiment-laden words in the English language. They also have different scales. We can browse the content of each dictionary as follows:

```{r}
library(tidytext)
head(get_sentiments("bing"))
```

Let's apply the `bing` sentiment dictionary to our database of tweets by Trump:

```{r, message=FALSE, warning=FALSE}

kamala_tweet_sentiment <- tidy_kamala_tweets %>%
  inner_join(get_sentiments("bing")) %>%
    count(created_at, sentiment) 

head(kamala_tweet_sentiment)
```

Now let's make a visual that compares the frequency of positive and negative tweets by day. To do this, we'll need to work a bit with the `created_at` variable---more specifically, we will need to transform it into a "date" object that we can use to pull out the day during which each tweet was made:

```{r}
library(lubridate)
tidy_kamala_tweets$date<-as_date(tidy_kamala_tweets$created_at)
```


*Question 8*: do you like working with date/time? what's your typical workflow when it comes to processing timestamps, date, etc? 

The `format` argument here tells R how to read in the date character string, since dates can appear in a number of different formats, time zones, etc. For more information about how to format data with other dates, see `?as.Date()`

Now let's aggregate negative sentiment by day

```{r}
kamala_sentiment_plot <-
  tidy_kamala_tweets %>%
    inner_join(get_sentiments("bing")) %>% 
      filter(sentiment=="positive") %>%
          count(date, sentiment)
```

Now, let's plot it:

```{r}
library(ggplot2)
ggplot(kamala_sentiment_plot, aes(x=date, y=n))+
  geom_line(color="red", size=.5)+
    theme_minimal()+
    theme(axis.text.x = 
            element_text(angle = 60, hjust = 1, size=13))+
    theme(plot.title = 
            element_text(hjust = 0.5, size=18))+
      ylab("Number of Negative Words")+
      xlab("")+
      ggtitle("Positive Sentiment in Kamala Tweets")+
      theme(aspect.ratio=1/4)
```


## When Should I use a Dictionary-Based Approach?

The quality of dictionary-based methods depends heavily upon the match between the learning-corpus and the one you want to code. Creating your own is often a good solution, but it is very time intensive. On the other hand, as we will see in future tutorials, dictionary-based approaches often perform better than more sophisticated techniques such as topic modeling, depending upon the task at hand.


